model,average_accuracy,agree_accuracy,czech_news_accuracy,klokanek_accuracy,ctkfacts_accuracy,average_validity,runtime[min]
gemini-1.5-pro-latest,0.7212500000000001,0.78,0.85,0.53,0.725,0.9962500000000001,
claude-3-5-sonnet-20241022,0.7187499999999999,0.855,0.845,0.48,0.695,0.9975,
claude-3-5-sonnet-20240620,0.7124999999999999,0.88,0.81,0.445,0.715,0.99875,
gpt-4o,0.6537499999999999,0.735,0.83,0.355,0.695,0.99625,
unsloth/Qwen2.5-72B-Instruct-bnb-4bit,0.64,0.58,0.86,0.445,0.675,1.0,17.324666559696198
Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4,0.6399999999999999,0.59,0.835,0.445,0.69,1.0,52.27458211183548
unsloth/Mistral-Large-Instruct-2407-bnb-4bit,0.63625,0.635,0.82,0.365,0.725,1.0,28.919193077087403
gpt-4-turbo,0.635,0.685,0.83,0.345,0.68,0.9975,
claude-3-5-haiku-20241022,0.63,0.64,0.84,0.31,0.73,1.0,
gemini-1.5-flash-latest,0.6174999999999999,0.625,0.815,0.34,0.69,0.9937499999999999,
Qwen/Qwen2.5-32B-Instruct,0.61375,0.65,0.785,0.4,0.62,1.0,6.078167967001597
google/gemma-2-27b-it,0.6,0.57,0.8,0.315,0.715,1.0,5.356812818845113
unsloth/Meta-Llama-3.1-70B-Instruct-bnb-4bit,0.5975,0.545,0.805,0.335,0.705,0.995,14.769054889678955
fsaudm/Meta-Llama-3.1-70B-Instruct-NF4,0.595,0.545,0.805,0.32,0.71,0.99375,14.813415431976319
claude-3-haiku-20240307,0.57625,0.575,0.795,0.285,0.65,1.0,
gemini-1.5-flash-8b-latest,0.57625,0.56,0.77,0.24,0.735,0.99875,
gpt-4o-mini,0.5750000000000001,0.585,0.805,0.31,0.6,0.995,
speakleash/Bielik-11B-v2.3-Instruct,0.57375,0.55,0.805,0.245,0.695,1.0,2.817930591106415
unsloth/c4ai-command-r-08-2024-bnb-4bit,0.5700000000000001,0.475,0.815,0.265,0.725,1.0,6.2932914853096005
AMead10/c4ai-command-r-08-2024-awq,0.5674999999999999,0.47,0.83,0.28,0.69,1.0,34.06181460618973
mistralai/Mistral-Small-Instruct-2409,0.5599999999999999,0.405,0.825,0.275,0.735,1.0,4.700380794207255
CohereForAI/aya-expanse-32b,0.5575,0.495,0.785,0.28,0.67,0.9975,4.406591665744782
google/gemma-2-9b-it,0.5325,0.485,0.785,0.25,0.61,1.0,2.84025156100591
gpt-3.5-turbo,0.53,0.485,0.765,0.28,0.59,1.0,
Qwen/Qwen2.5-7B-Instruct,0.52625,0.37,0.715,0.31,0.71,1.0,1.5650732080141703
CohereForAI/aya-23-35B,0.52125,0.485,0.76,0.235,0.605,0.99875,4.87549954255422
NousResearch/Hermes-3-Llama-3.1-8B,0.51875,0.435,0.695,0.26,0.685,1.0,1.4529012282689413
CohereForAI/aya-expanse-8b,0.515,0.485,0.76,0.23,0.585,1.0,1.3896692236264547
mistralai/Mixtral-8x7B-Instruct-v0.1,0.5125,0.295,0.76,0.265,0.73,0.985,35.97724781831106
mistralai/Ministral-8B-Instruct-2410,0.50875,0.39,0.685,0.285,0.675,1.0,1.5889750043551127
meta-llama/Llama-3.1-8B-Instruct,0.495,0.43,0.75,0.2,0.6,1.0,1.478528384367625
mistralai/Mistral-Nemo-Instruct-2407,0.49375,0.31,0.715,0.265,0.685,1.0,2.2253485282262164
google/gemma-2-2b-it,0.48624999999999996,0.43,0.64,0.305,0.57,1.0,1.2075307766596477
mistralai/Mistral-7B-Instruct-v0.3,0.47750000000000004,0.285,0.675,0.265,0.685,1.0,1.8032471458117167
meta-llama/Meta-Llama-3-8B-Instruct,0.475,0.31,0.715,0.29,0.585,1.0,1.45306582848231
CohereForAI/aya-23-8B,0.4625,0.24,0.71,0.28,0.62,1.0,1.3859365979830425
meta-llama/Llama-3.2-3B-Instruct,0.41125,0.315,0.5,0.3,0.53,1.0,0.8726231217384338
Qwen/Qwen2.5-3B-Instruct,0.40874999999999995,0.385,0.41,0.27,0.57,1.0,1.066944408416748
microsoft/Phi-3.5-mini-instruct,0.38625,0.185,0.57,0.24,0.55,1.0,2.11813827753067
Qwen/Qwen2.5-1.5B-Instruct,0.335,0.375,0.27,0.27,0.425,1.0,0.7319316784540812
unsloth/Llama-3.1-Nemotron-70B-Instruct-bnb-4bit,0.3325,0.0,0.695,0.015,0.62,0.46125,14.501169443130493
ibm-granite/granite-3.0-8b-instruct,0.33125,0.23,0.53,0.24,0.325,0.995,2.252877632776896
microsoft/Phi-3-mini-4k-instruct,0.3175,0.09,0.535,0.185,0.46,0.83,1.8970894773801168
microsoft/Phi-3-mini-128k-instruct,0.31125,0.13,0.62,0.0,0.495,0.6625,2.134305246671041
mistralai/Mistral-7B-Instruct-v0.1,0.30000000000000004,0.195,0.445,0.02,0.54,0.77125,1.8226038932800293
ibm-granite/granite-3.0-2b-instruct,0.21000000000000002,0.225,0.2,0.075,0.34,0.84375,1.1961600144704183
microsoft/Phi-3-medium-4k-instruct,0.135,0.315,0.0,0.225,0.0,0.41500000000000004,1.1106805841128031
microsoft/Phi-3-medium-128k-instruct,0.08625,0.195,0.0,0.15,0.0,0.23875000000000002,1.6416616757710776
meta-llama/Llama-3.2-1B-Instruct,0.06375,0.0,0.07,0.185,0.0,0.2075,0.4509873588879903
HuggingFaceTB/SmolLM-1.7B-Instruct,0.01625,0.0,0.0,0.065,0.0,0.05,0.7799754063288371
